{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melhem-m/AI-Training-Colab/blob/main/ENEC_rag_vector_stores_pineconeindexdemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
      "metadata": {
        "id": "307804a3-c02b-4a57-ac0d-172c30ddc851"
      },
      "source": [
        "# Pinecone Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36be66bf",
      "metadata": {
        "id": "36be66bf"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddff1e4",
      "metadata": {
        "id": "9ddff1e4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install llama-index llama-index-vector-stores-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48af8e1",
      "metadata": {
        "id": "d48af8e1"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
      "metadata": {
        "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396"
      },
      "source": [
        "#### Creating a Pinecone Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
      "metadata": {
        "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the OpenAI API key from Google Colab secrets\n",
        "openai.api_key = userdata.get('openai')\n",
        "\n",
        "if openai.api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "3mf6ed5pBFwj"
      },
      "id": "3mf6ed5pBFwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee",
      "metadata": {
        "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee"
      },
      "outputs": [],
      "source": [
        "#PUT Tyour api key\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "if api_key:\n",
        "    os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key"
      ],
      "metadata": {
        "id": "ftxZ6VHsVpsX"
      },
      "id": "ftxZ6VHsVpsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88",
      "metadata": {
        "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# dimensions are for text-embedding-ada-002\n",
        "\n",
        "pc.create_index(\n",
        "    name=\"quickstart\",\n",
        "    dimension=1536,\n",
        "    metric=\"euclidean\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6",
      "metadata": {
        "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6"
      },
      "outputs": [],
      "source": [
        "pinecone_index = pc.Index(\"quickstart\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
      "metadata": {
        "id": "8ee4473a-094f-4d0a-a825-e1213db07240"
      },
      "source": [
        "#### Load documents, build the PineconeVectorStore and VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2bcc07",
      "metadata": {
        "id": "0a2bcc07"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d782f76",
      "metadata": {
        "id": "7d782f76"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
      "metadata": {
        "id": "68cbd239-880e-41a3-98d8-dbb3fab55431"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "Jv678Qg8x1HG",
        "collapsed": true
      },
      "id": "Jv678Qg8x1HG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f07c3fd"
      },
      "source": [
        "The previous error occurred because the variable `pinecone_index` was used before it was defined. To fix this, I have combined the code that defines `pinecone_index` and the code that uses it into a single cell."
      ],
      "id": "5f07c3fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dd682a6"
      },
      "source": [
        "# Initialize Pinecone index\n",
        "pinecone_index = pc.Index(\"quickstart\")\n",
        "\n",
        "# initialize without metadata filter\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")"
      ],
      "id": "4dd682a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04304299-fc3e-40a0-8600-f50c3292767e",
      "metadata": {
        "id": "04304299-fc3e-40a0-8600-f50c3292767e"
      },
      "source": [
        "#### Query Index\n",
        "\n",
        "May take a minute or so for the index to be ready!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35369eda",
      "metadata": {
        "id": "35369eda"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "start_time = time.time()\n",
        "response = query_engine.query(\"what is this document about\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "# End timer and print duration\n",
        "end_time = time.time()\n",
        "print(f\"\\nExecution Time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
      "metadata": {
        "id": "bedbb693-725f-478f-be26-fa7180ea38b2"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "EgDEgB0_cOYo",
        "collapsed": true
      },
      "id": "EgDEgB0_cOYo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "# --- Initialize Pinecone ---\n",
        "\n",
        "index_name = \"quickstart\"\n",
        "dimension = 1536\n",
        "\n",
        "# Delete index if exists (optional: mirrors original behavior)\n",
        "if index_name in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "# Create Pinecone index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=dimension,\n",
        "    metric=\"euclidean\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n",
        "# --- Load Data ---\n",
        "# Create folders & download a sample doc (kept same logic, fixed subfolder creation)\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "# --- Create Index ---\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "\n",
        "# --- System Prompt (polite + answer-from-document constraint) ---\n",
        "SYSTEM_PROMPT = \"\"\"You are Aisha, a polite and professional SOP assistant.\n",
        "Answer ONLY using the information found in the indexed SOP document(s).\n",
        "If the answer is not in the document(s), say: \"I couldnâ€™t find that in the document.\"\n",
        "Keep responses concise, helpful, and courteous.\n",
        "\"\"\"\n",
        "\n",
        "# --- Query Engine ---\n",
        "query_engine = index.as_query_engine()\n",
        "def normalize_for_retrieval(q: str) -> str:\n",
        "    q0 = q.strip()\n",
        "    ql = q0.lower().strip()\n",
        "\n",
        "    # typo help\n",
        "    ql = ql.replace(\"safty\", \"safety\")\n",
        "\n",
        "    # If itâ€™s a \"who is X\" style question, add the name itself as a strong keyword hint\n",
        "    if ql.startswith(\"who is \"):\n",
        "        name = q0[7:].strip()              # take everything after \"who is \"\n",
        "        # Boost retrieval toward contact list patterns\n",
        "        return f'{name} {q0} Contact Information contacts list role title ext extension email phone'\n",
        "\n",
        "    # Generic contact-style boosting\n",
        "    contact_triggers = [\"extension\", \"ext\", \"contact\", \"phone\", \"email\", \"officer\", \"lead\", \"supervisor\", \"manager\"]\n",
        "    if any(t in ql for t in contact_triggers):\n",
        "        return f'{q0} Contact Information contacts list role title ext extension email phone'\n",
        "\n",
        "    return q0\n",
        "\n",
        "def query_doc(user_question: str):\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "    full_query = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "User question:\n",
        "{user_question.strip()}\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = query_engine.query(full_query)\n",
        "        text = str(response).strip()\n",
        "        # Gentle post-processing to keep it brief/polite\n",
        "        return text if text else \"I couldnâ€™t find that in the document.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# --- Gradio UI (Professional look with logo, centered title) ---\n",
        "# Use the raw GitHub URL for proper image rendering.\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/Decoding-Data-Science/Omantel/main/Omantel_Logo%20(1).png\"\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        ".gradio-container { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial; }\n",
        ".header-wrap {\n",
        "    display: grid;\n",
        "    grid-template-columns: 120px 1fr 120px;\n",
        "    align-items: center;\n",
        "    gap: 12px;\n",
        "    padding: 12px 0 8px;\n",
        "    border-bottom: 1px solid #eaeaea;\n",
        "}\n",
        ".header-logo { display:flex; align-items:center; justify-content:flex-start; }\n",
        ".header-logo img { height: 48px; object-fit: contain; }\n",
        ".header-title { text-align:center; }\n",
        ".header-title h1 {\n",
        "    margin: 0; font-weight: 700; font-size: 1.5rem; line-height: 1.2;\n",
        "}\n",
        ".header-spacer { height: 1px; }\n",
        ".section { padding-top: 8px; }\n",
        ".footer-note { text-align:center; font-size: 12px; color:#667085; padding: 8px 0 0; }\n",
        "label.svelte-1ipelgc, .label-wrap label { font-weight: 600; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=CUSTOM_CSS, title=\"SOP Document QA (LlamaIndex + Pinecone)\") as demo:\n",
        "    # Header with logo (left) and centered title\n",
        "    with gr.Row(elem_classes=\"header-wrap\"):\n",
        "        with gr.Column(scale=0, elem_classes=\"header-logo\"):\n",
        "            gr.HTML(f'<img src=\"{LOGO_URL}\" alt=\"Omantel Logo\" />')\n",
        "        with gr.Column(scale=1, elem_classes=\"header-title\"):\n",
        "            gr.HTML(\"<h1>SOP QA</h1>\")\n",
        "        with gr.Column(scale=0):\n",
        "            gr.HTML(\"\")  # right-side spacer\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"Ask questions based on the SOP Document \"\n",
        "        \"**Answers come only from the document**. If not found, Iâ€™ll say so.\"\n",
        "    )\n",
        "\n",
        "    with gr.Group(elem_classes=\"section\"):\n",
        "        inp = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"e.g., Ask in SOP Question?\",\n",
        "            lines=2,\n",
        "        )\n",
        "        btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        out = gr.Textbox(label=\"Answer\", lines=8)\n",
        "\n",
        "    btn.click(fn=query_doc, inputs=inp, outputs=out)\n",
        "    inp.submit(fn=query_doc, inputs=inp, outputs=out)\n",
        "\n",
        "    gr.Markdown('<div class=\"footer-note\">LlamaIndex + Pinecone â€¢ Demo</div>')\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "iZsatRldDujy"
      },
      "id": "iZsatRldDujy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicate the Deplpyment hugging face\n",
        "https://huggingface.co/spaces/decodingdatascience/ddsSOP1\n",
        "openai key\n"
      ],
      "metadata": {
        "id": "LFolx25Cj4sK"
      },
      "id": "LFolx25Cj4sK"
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "op8voXD_DENK",
        "collapsed": true
      },
      "id": "op8voXD_DENK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import userdata  # if you're in Colab; remove if not needed\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "# LlamaIndex OpenAI LLM\n",
        "# Install if needed:\n",
        "!pip install -q llama-index-llms-openai\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 0) Keys\n",
        "# ============================\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"Missing PINECONE_API_KEY (set in Colab userdata or env var).\")\n",
        "\n",
        "# Make sure you set OPENAI_API_KEY in Colab secrets or env vars\n",
        "OPENAI_API_KEY = userdata.get(\"openai\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY (set in Colab userdata or env var).\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 1) Initialize Pinecone\n",
        "# ============================\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "index_name = \"quickstart\"\n",
        "dimension = 1536  # keep as-is (matches common embedding dims)\n",
        "\n",
        "# Create the index ONLY if it doesn't exist\n",
        "existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
        "if index_name not in existing:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"euclidean\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2) (Optional) Load Data\n",
        "# ============================\n",
        "# If your document is ALREADY in Pinecone, you can keep this commented out.\n",
        "# If you want to (re)index local docs from ./data, uncomment and switch indexing below.\n",
        "# documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3) Connect to Existing Pinecone Vector Store\n",
        "# ============================\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# âœ… Use existing vectors already stored in Pinecone\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 4) LLM (OpenAI)\n",
        "# ============================\n",
        "Settings.llm = OpenAI(\n",
        "    model=\"gpt-5.2-2025-12-11\",\n",
        "    temperature=0,\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5) System Prompt (document-first constraint)\n",
        "# ============================\n",
        "SYSTEM_PROMPT = \"\"\"# SYSTEM PROMPT â€” STRICT DOCUMENT QA (CONTACT-AWARE)\n",
        "\n",
        "You answer ONLY using the DOCUMENT EXCERPTS provided below (these are retrieved from the Pinecone-indexed SOP). Never use outside knowledge. Never guess.\n",
        "\n",
        "VERY IMPORTANT:\n",
        "- Many questions in this SOP are answered in a \"Contact Information\" list. If the user asks about a role/title or a person, scan the excerpts for contact-list patterns like:\n",
        "  \"Safety Officer: <Name> â€“ Ext: <number>\"\n",
        "  \"<Role>: <Name> â€“ Ext: <number>\"\n",
        "  \"Email:\" \"Phone:\" \"Ext:\" \"Radio:\"\n",
        "- Treat these as equivalent intents:\n",
        "  - \"who is the safety officer\" = find the line that begins with \"Safety Officer:\"\n",
        "  - \"what is the extension of the safety officer\" = find the \"Safety Officer:\" line and extract \"Ext\"\n",
        "  - \"who is priya\" = find \"Priya\" in the excerpts and report her role and extension if shown\n",
        "- Be robust to small typos and variants: \"safty\"â†’\"safety\", \"ext\"â†’\"extension\".\n",
        "\n",
        "MANDATORY OUTPUT FORMAT (always):\n",
        "Reasoning: (1â€“2 sentences) Say what keyword(s) you looked for in the excerpts and QUOTE the exact matching line(s).\n",
        "Conclusion: Give the direct answer in 1 sentence.\n",
        "\n",
        "DENIAL RULE:\n",
        "Ifâ€”and only ifâ€”the excerpts contain no direct line that answers the question, reply exactly:\n",
        "\"I'm sorry, I cannot answer that as this information is not available in the document.\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Retrieval helpers (MODIFIED)\n",
        "# ============================\n",
        "# âœ… Increased recall to avoid needing page numbers\n",
        "retriever = index.as_retriever(similarity_top_k=15)\n",
        "\n",
        "def expand_query(q: str) -> str:\n",
        "    q0 = q.strip()\n",
        "    ql = q0.lower().strip()\n",
        "\n",
        "    # typo help\n",
        "    ql = ql.replace(\"safty\", \"safety\")\n",
        "\n",
        "    # If itâ€™s a \"who is X\" style question, add the name itself as a strong keyword hint\n",
        "    if ql.startswith(\"who is \"):\n",
        "        name = q0[7:].strip()\n",
        "        return (\n",
        "            f\"{name} {q0} \"\n",
        "            \"Contact Information contacts list role title ext extension email phone\"\n",
        "        )\n",
        "\n",
        "    # Generic contact-style boosting\n",
        "    contact_triggers = [\n",
        "        \"extension\", \"ext\", \"contact\", \"phone\", \"email\",\n",
        "        \"officer\", \"lead\", \"supervisor\", \"manager\", \"maintenance\", \"safety\"\n",
        "    ]\n",
        "    if any(t in ql for t in contact_triggers):\n",
        "        return (\n",
        "            f\"{q0} \"\n",
        "            \"Contact Information contacts list role title ext extension email phone\"\n",
        "        )\n",
        "\n",
        "    return q0\n",
        "\n",
        "\n",
        "def query_doc(user_question: str):\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    q = user_question.strip()\n",
        "\n",
        "    # 1) Retrieve relevant excerpts FIRST (with expanded query)\n",
        "    nodes = retriever.retrieve(expand_query(q))\n",
        "\n",
        "    if not nodes:\n",
        "        return \"I'm sorry, I cannot answer that as this information is not available in the document.\"\n",
        "\n",
        "    # Build a compact context from top retrieved chunks\n",
        "    excerpts = []\n",
        "    for i, n in enumerate(nodes[:5], start=1):\n",
        "        txt = (n.get_content() or \"\").strip()\n",
        "        if txt:\n",
        "            excerpts.append(f\"[Excerpt {i}]\\n{txt}\")\n",
        "\n",
        "    context_block = \"\\n\\n\".join(excerpts).strip()\n",
        "\n",
        "    if not context_block:\n",
        "        return \"I'm sorry, I cannot answer that as this information is not available in the document.\"\n",
        "\n",
        "    # 2) Ask the LLM to answer ONLY from these excerpts\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "DOCUMENT EXCERPTS:\n",
        "{context_block}\n",
        "\n",
        "USER QUESTION:\n",
        "{q}\n",
        "\n",
        "ANSWER (follow the rules exactly):\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = Settings.llm.complete(prompt)\n",
        "        text = (resp.text or \"\").strip()\n",
        "\n",
        "        if not text:\n",
        "            return \"I'm sorry, I cannot answer that as this information is not available in the document.\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 6) Gradio UI\n",
        "# ============================\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/melhem-m/AI-Training-Colab/main/enec%20logo.png\"\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        ".gradio-container { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial; }\n",
        ".header-wrap {\n",
        "    display: grid;\n",
        "    grid-template-columns: 72px 1fr;\n",
        "    align-items: center;\n",
        "    gap: 12px;\n",
        "    padding: 12px 0 10px;\n",
        "    border-bottom: 1px solid #eaeaea;\n",
        "}\n",
        ".header-logo img { height: 44px; width: 44px; object-fit: contain; border-radius: 10px; }\n",
        ".header-title h1 { margin: 0; font-weight: 800; font-size: 1.35rem; line-height: 1.2; }\n",
        ".subtle { color:#667085; font-size: 0.95rem; margin-top: 4px; }\n",
        ".footer-note { text-align:center; font-size: 12px; color:#667085; padding: 10px 0 0; }\n",
        "label.svelte-1ipelgc, .label-wrap label { font-weight: 650; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=CUSTOM_CSS, title=\"SOP Document QA (LlamaIndex + Pinecone + OpenAI)\") as demo:\n",
        "    with gr.Row(elem_classes=\"header-wrap\"):\n",
        "        gr.HTML(f'<div class=\"header-logo\"><img src=\"{LOGO_URL}\" alt=\"SOP Logo\" /></div>')\n",
        "        gr.HTML('<div class=\"header-title\"><h1>SOP QA</h1><div class=\"subtle\">Answers come only from the document. If not found, Iâ€™ll say so.</div></div>')\n",
        "\n",
        "    with gr.Group():\n",
        "        inp = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"e.g., Who is the safety officer? What is their extension?\",\n",
        "            lines=2,\n",
        "        )\n",
        "        btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        out = gr.Textbox(label=\"Answer\", lines=10)\n",
        "\n",
        "    btn.click(fn=query_doc, inputs=inp, outputs=out)\n",
        "    inp.submit(fn=query_doc, inputs=inp, outputs=out)\n",
        "\n",
        "    gr.Markdown('<div class=\"footer-note\">LlamaIndex + Pinecone â€¢ OpenAI LLM â€¢ Demo</div>')\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "JBGgBjCA3nWm"
      },
      "id": "JBGgBjCA3nWm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}